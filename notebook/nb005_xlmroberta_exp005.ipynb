{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"nb005_xlmroberta_exp005.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1HcM9NVWkknuR3oS1drfzipYNh6yBdi2y","authorship_tag":"ABX9TyNuXz3WQJJ5/gjo4b6LGRO2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"h7PiERPR3JwG"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1uHWvJoM1MB"},"source":["!mkdir ~/.kaggle\n","!cp /content/drive/MyDrive/Colab/kaggle/kaggle.json ~/.kaggle\n","\n","!pip install --upgrade --force-reinstall --no-deps kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Urj-Rfgn3UBV"},"source":["!cp -r /content/drive/MyDrive/Colab/kaggle/chaii-hindi-and-tamil-question-answering/input /content\n","!mkdir /content/checkpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6dGMLhJ3f0r"},"source":["!pip install -U pytorch-lightning transformers wandb sentencepiece torchsummaryX"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGc_0a3l32o8"},"source":["import os\n","import random\n","from dataclasses import dataclass\n","import copy\n","\n","import numpy as np\n","import pandas as pd\n","import math\n","import sklearn.model_selection as sms\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n","import torch.optim as optim\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer, seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n","from pytorch_lightning.loggers import WandbLogger\n","import wandb\n","\n","from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n","from torchsummaryX import summary\n","\n","from collections import defaultdict, OrderedDict\n","from tqdm.notebook import tqdm\n","\n","import json\n","from pathlib import Path\n","from kaggle.api.kaggle_api_extended import KaggleApi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2pbFM7_B4Emp"},"source":["## config"]},{"cell_type":"code","metadata":{"id":"HemZ1ng-4FIW"},"source":["@dataclass\n","class Config:\n","    #########################\n","    # Globals #\n","    #########################\n","    exp_name = \"exp005\"\n","    debug = False\n","    gpus = 1\n","    num_workers = 8\n","    num_epochs = 1\n","    grad_accumulate = 3\n","    fp16 = False\n","    seed = [42, 2434, 98, 24, 1991]\n","    #########################\n","    # Data #\n","    #########################\n","    train_csv = \"input/train.csv\"\n","    test_csv = \"input/test.csv\"\n","    sample_submission_csv = \"input/sample_submission.csv\"\n","    mlqa_hindi_csv = \"input/external_data/mlqa_hindi.csv\"\n","    xquad_hindi_csv = \"input/external_data/xquad_hindi.csv\"\n","    checkpoint_dir = \"checkpoint\"\n","    #########################\n","    # Split #\n","    #########################\n","    split_name = \"StratifiedKFold\"\n","    split_params = {\n","        \"n_splits\": 5,\n","        \"random_state\": 42,\n","        \"shuffle\": True,\n","        }\n","    #########################\n","    # Tokenizer #\n","    #########################\n","    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n","    max_seq_length = 384\n","    doc_stride = 128\n","    truncation = \"only_second\"\n","    padding = \"max_length\"\n","    #########################\n","    # Dataset #\n","    #########################\n","    #########################\n","    # DataLoader #\n","    #########################\n","    train_batch_size = 4\n","    valid_batch_size = 16\n","    test_batch_size = 128\n","    #########################\n","    # Model #\n","    #########################\n","    base_model_name = \"deepset/xlm-roberta-large-squad2\"\n","    base_model_config = \"deepset/xlm-roberta-large-squad2\"\n","    num_classes = 2\n","    init_layers = 1\n","    #########################\n","    # Criterion #\n","    #########################\n","    loss_name = \"CrossEntropyLoss\"\n","    loss_params = {\n","        \"ignore_index\": -1,\n","    }\n","    #########################\n","    # Optimizer #\n","    #########################\n","    optimizer_name = \"AdamW\"\n","    optimizer_params = {\n","        \"lr\": 1.5e-5,\n","        \"weight_decay\": 1e-2,\n","        \"eps\": 1e-8,\n","        \"correct_bias\": True\n","    }\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    header_weight_decay = 1e-2\n","    header_lr = 1e-3\n","    lr_decay = 0.98\n","    ######################\n","    # Scheduler #\n","    ######################\n","    scheduler_name = \"linear-warmup\"\n","    scheduler_params = {\n","        \"warmup_ratio\": 0.1,\n","    }\n","    ######################\n","    # Callbacks #\n","    ######################\n","    model_checkpoint_params = {\n","        \"monitor\": \"val/jaccard_epoch\",\n","        \"save_top_k\": 1,\n","        \"save_weights_only\": True,\n","    }\n","    early_stopping_params = {\n","        \"monitor\": \"val/jaccard_epoch\",\n","        \"min_delta\": 0.0,\n","        \"patience\": 5,\n","        \"verbose\": False,\n","        \"mode\": \"max\",\n","    }\n","    wandb_logger_params = {\n","        \"project\": \"kaggle-chaii-hindi-and-tamil-question-answering\"\n","    }\n","\n","\n","cfg = Config()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N75ldQ_N4_lF"},"source":["## utils"]},{"cell_type":"code","metadata":{"id":"N1OPGpKZ48h-"},"source":["def load_dataset(cfg):\n","    train_df = pd.read_csv(cfg.train_csv, nrows= 100 if cfg.debug else None)\n","    test_df = pd.read_csv(cfg.test_csv)\n","    external_mlqa_df = pd.read_csv(cfg.mlqa_hindi_csv)\n","    external_xquad_df = pd.read_csv(cfg.xquad_hindi_csv)\n","    external_train_df = pd.concat([external_mlqa_df, external_xquad_df], axis=0).reset_index(drop=True)\n","    sample_submission_df = pd.read_csv(cfg.sample_submission_csv)\n","\n","    return train_df, test_df, external_train_df, sample_submission_df\n","\n","\n","def get_split(cfg):\n","    split_name = cfg.split_name\n","    split_params = cfg.split_params\n","\n","    return sms.__getattribute__(split_name)(**split_params)\n","\n","\n","def get_fold(cfg, train_df: pd.DataFrame, y_train: pd.DataFrame):\n","    splitter = get_split(cfg)\n","    train_df[\"fold\"] = -1\n","    for fold_id, (train_idx, valid_idx) in enumerate(splitter.split(train_df, y_train)):\n","        train_df.loc[valid_idx, \"fold\"] = int(fold_id)\n","\n","    return train_df\n","\n","\n","def convert_answers(row):\n","    return {\"answer_start\": [row[0]], \"text\": [row[1]]}\n","\n","\n","def jaccard(str1, str2):\n","    a = set(str1.lower().split())\n","    b = set(str2.lower().split())\n","    c = a.intersection(b)\n","\n","    return float(len(c)) / (len(a) + len(b) - len(c))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RBsFiLLN5EVr"},"source":["## load data"]},{"cell_type":"code","metadata":{"id":"AwmhgEfk4vR8"},"source":["train_df, test_df, external_train_df, sample_submission_df = load_dataset(cfg)\n","\n","train_df = get_fold(cfg, train_df, train_df[\"language\"])\n","\n","external_train_df[\"fold\"] = -1\n","external_train_df[\"id\"] = list(np.arange(1, len(external_train_df) + 1))\n","train_df = pd.concat([train_df, external_train_df], axis=0).reset_index(drop=True)\n","\n","train_df[\"answers\"] = train_df[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)\n","train_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WcJnS8s25JLn"},"source":["## preprocess"]},{"cell_type":"code","metadata":{"id":"nxgeHj2A82w0"},"source":["def prepare_train_features(cfg, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=cfg.truncation,\n","        max_length=cfg.max_seq_length,\n","        stride=cfg.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=cfg.padding,\n","        # return_tensors=\"pt\"\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids  = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature[\"input_ids\"] = input_ids\n","        feature[\"attention_mask\"] = attention_mask\n","        feature[\"offset_mapping\"] = offsets\n","        feature[\"example_id\"] = example[\"id\"]\n","        feature[\"sequence_ids\"] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id) # cls_token_id = 0, pad_token_id = 1\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        # sample_indx = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0: # sampleにanswerがなければ開始・終了位置をCLS(=[CLS], <s>)にする\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0] # answer部分の文字レベルの開始位置\n","            end_char = start_char + len(answers[\"text\"][0]) # answer部分の文字レベルの終了位置\n","\n","            token_start_index = 0 # sequenceにおけるcontext部分のトークンレベルの開始位置\n","            while sequence_ids[token_start_index] != 1: # sequence_idsが1(=context部分)になるまで足す(<s>,</s>, querstion部分を飛ばすイメージ)\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1 # sequenceにおけるcontext部分のトークンレベルの終了位置\n","            while sequence_ids[token_end_index] != 1: # sequence_idsが1(=context部分)になるまで引く(paddingはNoneなので<pad>部分を引くイメージ)\n","                token_end_index -= 1\n","\n","            # token_start_index, token_end_indexがanswer部分の範囲外にあればpositionを0(=[CLS], <s>)とする(truncationされてる場合はこっち)\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            # star_char, end_charを含むtokenまでtoken_start_index, token_end_indexを調整\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1 # 行き過ぎた1token分戻す\n","\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1 # 行き過ぎた1token分戻す\n","\n","        features.append(feature)\n","\n","    return features\n","\n","\n","def prepare_test_features(cfg, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=cfg.truncation,\n","        max_length=cfg.max_seq_length,\n","        stride=cfg.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=cfg.padding,\n","        # return_tensors=\"pt\"\n","    )\n","\n","    features = []\n","    for i in range(len(tokenized_example[\"input_ids\"])):\n","        feature = {}\n","        feature[\"example_id\"] = example[\"id\"]\n","        feature[\"context\"] = example[\"context\"]\n","        feature[\"question\"] = example[\"question\"]\n","        feature[\"input_ids\"] = tokenized_example[\"input_ids\"][i]\n","        feature[\"attention_mask\"] = tokenized_example[\"attention_mask\"][i]\n","        feature[\"offset_mapping\"] = tokenized_example[\"offset_mapping\"][i]\n","        feature[\"sequence_ids\"] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n","        features.append(feature)\n","\n","    return features\n","\n","\n","def postprocess_qa_predictions(tokenizer, examples: pd.DataFrame, features, raw_predictions, n_best_size=20, max_answer_length=30):\n","    '''予測値の後処理関数\n","    '''\n","    all_start_logits, all_end_logits = raw_predictions\n","\n","    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])} # dataframeのidをkeyとしたindexのmapping用dict\n","    features_per_example = defaultdict(list) # dataframeのidに対応するexample_id_to_indexのidをkeyとしたdict。valueはfeaturesに対応するidのリスト\n","    for i, feature in enumerate(features):\n","        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","    predictions = OrderedDict()\n","    # print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n","\n","    for example_index, example in examples.iterrows():\n","        feature_indices = features_per_example[example_index] # 対応するfeaturesのindexを取り出す\n","\n","        # min_null_score = None\n","        valid_answers = []\n","\n","        context = example[\"context\"]\n","        for feature_index in feature_indices:\n","            start_logits = all_start_logits[feature_index]\n","            end_logits = all_end_logits[feature_index]\n","\n","            sequence_ids = features[feature_index][\"sequence_ids\"]\n","            context_index = 1 # contextのsequence_id(questionは0)\n","            \n","            # contextのみoffset_mappingを保持(questionのoffset_mappingをNoneに)\n","            features[feature_index][\"offset_mapping\"] = [\n","                (o if sequence_ids[k] == context_index else None)\n","                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n","            ]\n","            offset_mapping = features[feature_index][\"offset_mapping\"]\n","            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n","\n","            # feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n","            # if min_null_score is None or min_null_score < feature_null_score:\n","            #     min_null_score = feature_null_score\n","\n","            # start_indexes = np.argsort(start_logits)[-1: -n_best_size - 1: -1].tolist()\n","            # end_indexes = np.argsort(end_logits)[-1: -n_best_size - 1: -1].tolist()\n","            start_indexes = np.argsort(start_logits)[::-1][:n_best_size].tolist()\n","            end_indexes = np.argsort(end_logits)[::-1][:n_best_size].tolist()\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    if (\n","                        start_index >= len(offset_mapping)\n","                        or end_index >= len(offset_mapping)\n","                        or offset_mapping[start_index] is None\n","                        or offset_mapping[end_index] is None\n","                    ):\n","                        continue\n","                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n","                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n","                        continue\n","\n","                    start_char = offset_mapping[start_index][0]\n","                    end_char = offset_mapping[end_index][1]\n","                    valid_answers.append(\n","                        {\n","                            \"score\": start_logits[start_index] + end_logits[end_index],\n","                            \"text\": context[start_char: end_char]\n","                        }\n","                    )\n","        # 各レコード・チャンク(feature)におけるstart+end出力値のスコアが最も大きいペアを最終的な予測値とする\n","        if len(valid_answers) > 0:\n","            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n","        else:\n","            best_answer = {\"text\": \"\", \"score\": 0.0}\n","\n","        predictions[example[\"id\"]] = best_answer[\"text\"]\n","\n","    return predictions\n","\n","\n","def postprocess_cleaned_predictions(input_df: pd.DataFrame):\n","    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\n","    bad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n","\n","    tamil_ad = \"கி.பி\"\n","    tamil_bc = \"கி.மு\"\n","    tamil_km = \"கி.மீ\"\n","    hindi_ad = \"ई\"\n","    hindi_bc = \"ई.पू\"\n","\n","\n","    cleaned_preds = []\n","    for pred, context in input_df[[\"PredictionString\", \"context\"]].to_numpy():\n","        if pred == \"\":\n","            cleaned_preds.append(pred)\n","            continue\n","        while any([pred.startswith(y) for y in bad_starts]):\n","            pred = pred[1:]\n","        while any([pred.endswith(y) for y in bad_endings]):\n","            if pred.endswith(\"...\"):\n","                pred = pred[:-3]\n","            else:\n","                pred = pred[:-1]\n","        if pred.endswith(\"...\"):\n","                pred = pred[:-3]\n","\n","        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n","            pred = pred+\".\"\n","\n","        cleaned_preds.append(pred)\n","        \n","    return cleaned_preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3nn0RX0BicrZ"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"OHSxITfLa9y8"},"source":["class ChaiiDataset(Dataset):\n","    def __init__(self, features, phase):\n","        super(ChaiiDataset, self).__init__()\n","        self.features = features\n","        self.phase = phase\n","        \n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        feature = self.features[index]\n","\n","        if self.phase == \"train\":\n","            return {\n","                \"input_ids\": torch.tensor(feature[\"input_ids\"], dtype=torch.long),\n","                \"attention_mask\": torch.tensor(feature[\"attention_mask\"], dtype=torch.long),\n","                \"offset_mapping\": torch.tensor(feature[\"offset_mapping\"], dtype=torch.long),\n","                \"start_position\": torch.tensor(feature[\"start_position\"], dtype=torch.long),\n","                \"end_position\": torch.tensor(feature[\"end_position\"], dtype=torch.long),\n","            }\n","        else:\n","            return {\n","                \"input_ids\": torch.tensor(feature[\"input_ids\"], dtype=torch.long),\n","                \"attention_mask\": torch.tensor(feature[\"attention_mask\"], dtype=torch.long),\n","                \"offset_mapping\": torch.tensor(feature[\"offset_mapping\"], dtype=torch.long),\n","                \"sequence_ids\": feature[\"sequence_ids\"],\n","                \"id\": feature[\"example_id\"],\n","                \"context\": feature[\"context\"],\n","                \"question\": feature[\"question\"],\n","            }\n","\n","\n","class ChaiiDataModule(pl.LightningDataModule):\n","    def __init__(self, cfg, tokenizer, input_df: pd.DataFrame, phase: str, fold: int = 0):\n","        super(ChaiiDataModule, self).__init__()\n","        self.cfg = cfg\n","        self.tokenizer = tokenizer\n","        self.input_df = input_df\n","        self.phase = phase\n","        self.fold = fold\n","\n","    def get_train_features(self, cfg, input_df, tokenizer):\n","        features = []\n","        for i, row in tqdm(input_df.iterrows(), total=len(input_df), desc=\"[get features]\"):\n","            features += prepare_train_features(cfg, row, tokenizer)\n","        \n","        return features\n","\n","    def get_test_features(self, cfg, input_df, tokenizer):\n","        features = []\n","        for i, row in tqdm(input_df.iterrows(), total=len(input_df), desc=\"[get features]\"):\n","            features += prepare_test_features(cfg, row, tokenizer)\n","        \n","        return features\n","\n","    def setup(self, stage=None):\n","        assert self.phase in (\"train\", \"test\"), \"Input phase is not exist.\"\n","        if self.phase == \"train\":\n","            self.train_df = self.input_df[self.input_df[\"fold\"] != self.fold].reset_index(drop=True)\n","            self.valid_df = self.input_df[self.input_df[\"fold\"] == self.fold].reset_index(drop=True)\n","\n","            self.train_features = self.get_train_features(self.cfg, self.train_df, self.tokenizer)\n","            self.valid_features = self.get_train_features(self.cfg, self.valid_df, self.tokenizer)\n","\n","            self.train_dataset = ChaiiDataset(self.train_features, self.phase)\n","            self.valid_dataset = ChaiiDataset(self.valid_features, self.phase)\n","            print(f\"Number of train features: {len(self.train_dataset)}, Number of valid features: {len(self.valid_dataset)}\")\n","        elif self.phase == \"test\":\n","            self.test_features = self.get_test_features(self.cfg, self.input_df, self.tokenizer)\n","            self.test_dataset = ChaiiDataset(self.test_features, self.phase)\n","            print(f\"Number of test features: {len(self.test_dataset)}\")\n","        else:\n","            raise NotImplementedError\n","        \n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.cfg.train_batch_size, num_workers=self.cfg.num_workers, pin_memory=True, shuffle=True, drop_last=False)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.valid_dataset, batch_size=self.cfg.valid_batch_size, num_workers=self.cfg.num_workers, pin_memory=True, shuffle=False, drop_last=False)\n","\n","    def predict_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.cfg.test_batch_size, num_workers=self.cfg.num_workers, pin_memory=True, shuffle=False, drop_last=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HhzSnmitfua7"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"IcStqJUDfees"},"source":["class ChaiiModel(nn.Module):\n","    def __init__(self, cfg):\n","        super(ChaiiModel, self).__init__()\n","        self.cfg = cfg\n","        self.model_config = AutoConfig.from_pretrained(self.cfg.base_model_config)\n","        self.encoder = AutoModel.from_pretrained(self.cfg.base_model_name, config=self.model_config)\n","        self.classifier = nn.Linear(self.model_config.hidden_size, cfg.num_classes)\n","        self._init_header_weights(self.classifier)\n","        # self._init_roberta_weights(self.encoder)\n","\n","    def _init_header_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def _init_roberta_weights(self, model):\n","        for layer in model.encoder.layer[-cfg.init_layers:]:\n","            for module in layer.modules():\n","                if isinstance(module, nn.Linear):\n","                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n","                    if module.bias is not None:\n","                        module.bias.data.zero_()\n","                elif isinstance(module, nn.Embedding):\n","                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n","                    if module.padding_idx is not None:\n","                        module.weight.data[module.padding_idx].zero_()\n","                elif isinstance(module, nn.LayerNorm):\n","                    module.bias.data.zero_()\n","                    module.weight.data.fill_(1.0)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        x = self.classifier(output[0]) # (batch_size, hidden_size, num_classes)\n","        x0, x1 = x.split(1, dim=-1) # (batch_size, hidden_size, 1)\n","        start_logits, end_logits = x0.squeeze(-1), x1.squeeze(-1) # (batch_size, hidden_size)\n","\n","        return start_logits, end_logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZ15q-gb5DiO"},"source":["## training"]},{"cell_type":"code","metadata":{"id":"8mKGz0xAqy3L"},"source":["def get_optimizer_grouped_parameters(cfg, model):\n","    no_decay = cfg.no_decay\n","    # header layerのweight_decay, lr\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if \"classifier\" in n],\n","            \"weight_decay\": cfg.header_weight_decay,\n","            \"lr\": cfg.header_lr\n","        },\n","    ]\n","\n","    # num_layers = model.base_model.config.num_hidden_layers\n","    layers = [getattr(model, \"base_model\").embeddings] + list(getattr(model, \"base_model\").encoder.layer)\n","    layers.reverse()\n","    lr = cfg.optimizer_params[\"lr\"]\n","    \n","    for layer in layers:\n","        lr *= cfg.lr_decay\n","        optimizer_grouped_parameters += [\n","            # no_decayのリストに含まれないパラメータはweight decayを設定\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": cfg.optimizer_params[\"weight_decay\"],\n","                \"lr\": lr,\n","            },\n","            # no_decayのリストに含まれるパラメータはweight decayを設定しない\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\": lr,\n","            }\n","        ]\n","    \n","    return optimizer_grouped_parameters\n","\n","\n","def get_criterion(cfg):\n","    loss_name = cfg.loss_name\n","    loss_params = cfg.loss_params\n","    return nn.__getattribute__(loss_name)(**loss_params)\n","\n","\n","def get_optimizer(cfg, model=None, optimizer_grouped_parameters=None):\n","    optimizer_name = cfg.optimizer_name\n","    optimizer_params = cfg.optimizer_params\n","\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": cfg.optimizer_params[\"weight_decay\"],\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","\n","    if optimizer_name == \"AdamW\":\n","        return AdamW(\n","            optimizer_grouped_parameters,\n","            **optimizer_params\n","        )\n","    else:\n","        return optim.__getattribute__(optimizer_name)(model.parameters(), **optimizer_params)\n","\n","\n","def get_scheduler(cfg, optimizer, num_warmup_steps=None, num_training_steps=None):\n","    scheduler_name = cfg.scheduler_name\n","    scheduler_params = cfg.scheduler_params\n","\n","    if scheduler_name == \"cosine-warmup\":\n","        return get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","        )\n","    elif scheduler_name == \"linear-warmup\":\n","        return get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","        )\n","    elif scheduler_name not in (\"cosine-warmup\", \"linear-warmup\"):\n","        return optim.lr_scheduler.__getattribute__(scheduler_name)(optimizer, **scheduler_params)\n","    elif scheduler_name is not None:\n","        return\n","\n","\n","class ChaiiLightningModule(pl.LightningModule):\n","    def __init__(self, cfg):\n","        super(ChaiiLightningModule, self).__init__()\n","        self.cfg = cfg\n","        self.model = ChaiiModel(self.cfg)\n","        self.criterion = get_criterion(self.cfg)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output_start, output_end = self.model(input_ids, attention_mask)\n","        return output_start, output_end\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask, target_start, target_end = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"start_position\"], batch[\"end_position\"]\n","        output_start, output_end = self.forward(input_ids, attention_mask)\n","\n","        loss_start = self.criterion(output_start, target_start)\n","        loss_end = self.criterion(output_end, target_end)\n","        loss = (loss_start + loss_end) / 2\n","\n","        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"train/loss_start_epoch\", loss_start, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"train/loss_end_epoch\", loss_end, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids, attention_mask, target_start, target_end = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"start_position\"], batch[\"end_position\"]\n","        output_start, output_end = self.forward(input_ids, attention_mask)\n","\n","        loss_start = self.criterion(output_start, target_start)\n","        loss_end = self.criterion(output_end, target_end)\n","        loss = (loss_start + loss_end) / 2\n","\n","        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"val/loss_start_epoch\", loss_start, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"val/loss_end_epoch\", loss_end, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","        outputs = OrderedDict({\n","            \"start_logits\": output_start.detach(),\n","            \"end_logits\": output_end.detach(),\n","        }\n","        )\n","\n","        return outputs\n","\n","    def validation_epoch_end(self, outputs):\n","        pred_start = torch.cat([output[\"start_logits\"] for output in outputs]).cpu().numpy()\n","        pred_end = torch.cat([output[\"end_logits\"] for output in outputs]).cpu().numpy()\n","\n","        preds = postprocess_qa_predictions(\n","            self.trainer.datamodule.tokenizer,\n","            self.trainer.datamodule.valid_df,\n","            copy.deepcopy(self.trainer.datamodule.valid_features[:pred_start.shape[0]]),\n","            (pred_start, pred_end)\n","        )\n","        jaccard_score = np.mean([jaccard(x, y) for x, y in zip(self.trainer.datamodule.valid_df[\"answer_text\"].values, preds.values())], axis=0)\n","\n","        self.log(\"val/jaccard_epoch\", jaccard_score, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n","        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n","        output_start, output_end = self.forward(input_ids, attention_mask)\n","\n","        outputs = OrderedDict({\n","            \"start_logits\": output_start,\n","            \"end_logits\": output_end,\n","        }\n","        )\n","\n","        return outputs\n","\n","    def configure_optimizers(self):\n","        # optimizer_grouped_parameters = get_optimizer_grouped_parameters(self.cfg, self.model)\n","        optimizer = get_optimizer(\n","            self.cfg,\n","            self.model,\n","            # optimizer_grouped_parameters=optimizer_grouped_parameters\n","        )\n","\n","        num_training_steps = math.ceil(len(self.trainer.datamodule.train_dataloader()) / self.trainer.accumulate_grad_batches) * self.trainer.max_epochs\n","        if self.cfg.scheduler_params[\"warmup_ratio\"] > 0:\n","            num_warmup_steps = int(num_training_steps * self.cfg.scheduler_params[\"warmup_ratio\"])\n","        else:\n","            num_warmup_steps = 0\n","        scheduler = get_scheduler(self.cfg, optimizer, num_warmup_steps, num_training_steps)\n","        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n","        print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","\n","        return [optimizer], [scheduler]\n","\n","\n","def run_fold(cfg, train_df, fold, tokenizer):\n","    seed_everything(cfg.seed[0])\n","\n","    checkpoint_callback = ModelCheckpoint(\n","        dirpath=cfg.checkpoint_dir,\n","        filename=f\"{cfg.exp_name}_fold_{fold}\",\n","        **cfg.model_checkpoint_params,\n","    )\n","\n","    early_stopping_callback = EarlyStopping(**cfg.early_stopping_params)\n","\n","    wandb_logger = WandbLogger(\n","        name=f\"{cfg.exp_name}_fold_{fold}\",\n","        **cfg.wandb_logger_params,\n","    )\n","\n","    trainer = Trainer(\n","        default_root_dir=cfg.checkpoint_dir,\n","        gpus=cfg.gpus,\n","        max_epochs=cfg.num_epochs,\n","        accumulate_grad_batches=cfg.grad_accumulate,\n","        precision=16 if cfg.fp16 else 32,\n","        callbacks=[\n","            checkpoint_callback,\n","            # early_stopping_callback,\n","        ],\n","        logger=[\n","            # wandb_logger,\n","        ],\n","        log_every_n_steps=10,\n","    )\n","\n","    model = ChaiiLightningModule(cfg)\n","    datamodule = ChaiiDataModule(cfg, tokenizer=tokenizer, input_df=train_df, phase=\"train\", fold=fold)\n","    trainer.fit(model, datamodule=datamodule)\n","\n","    wandb.finish()\n","\n","    return checkpoint_callback.best_model_path, checkpoint_callback.best_model_score.item()\n","\n","\n","def run_training(cfg, train_df):\n","    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n","\n","    checkpoint_path_list = []\n","    oof_score = 0\n","\n","    for fold_id in range(cfg.split_params[\"n_splits\"]):\n","        checkpoint_path, best_score = run_fold(cfg, train_df, fold_id, tokenizer)\n","        checkpoint_path_list.append(checkpoint_path)\n","        oof_score += best_score / int(cfg.split_params[\"n_splits\"])\n","\n","    print(\"CV jaccard score :\", oof_score)\n","\n","    return checkpoint_path_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhGIfEKXBatS"},"source":["# checkpoint_path_list = run_training(cfg, train_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbsE5t9-jbB3"},"source":["# ID = \"azupero\"\n","# DATASET_ID = f\"chaii-qa-checkpoint-{cfg.exp_name}\"\n","# UPLOAD_DIR = Path(cfg.checkpoint_dir)\n","\n","# def dataset_create_new():\n","#     dataset_metadata = {}\n","#     dataset_metadata['id'] = f'{ID}/{DATASET_ID}'\n","#     dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","#     dataset_metadata['title'] = DATASET_ID\n","#     with open(UPLOAD_DIR / 'dataset-metadata.json', 'w') as f:\n","#         json.dump(dataset_metadata, f, indent=4)\n","#     api = KaggleApi()\n","#     api.authenticate()\n","#     api.dataset_create_new(folder=UPLOAD_DIR, convert_to_csv=False, dir_mode='tar')\n","\n","# dataset_create_new()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USegY8kE1TMN"},"source":["## inference"]},{"cell_type":"code","metadata":{"id":"a2iSNKOK7-32"},"source":["def run_predict(cfg, input_df):\n","    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n","    checkpoint_path = list(Path(cfg.checkpoint_dir).iterdir())\n","    start_logits = []\n","    end_logits = []\n","\n","    for path in checkpoint_path:\n","        seed_everything(cfg.seed[0])\n","\n","        model = ChaiiLightningModule.load_from_checkpoint(checkpoint_path=path, cfg=cfg)\n","        datamodule = ChaiiDataModule(cfg, tokenizer=tokenizer, input_df=input_df, phase=\"test\")\n","        trainer = Trainer(gpus=cfg.gpus)\n","        preds = trainer.predict(model, datamodule=datamodule, return_predictions=True)\n","        start_logits.append(torch.cat([pred[\"start_logits\"] for pred in preds]).cpu().numpy())\n","        end_logits.append(torch.cat([pred[\"end_logits\"] for pred in preds]).cpu().numpy())\n","\n","    start_logits, end_logits = np.mean(start_logits, axis=0), np.mean(end_logits, axis=0)\n","    predictions = postprocess_qa_predictions(input_df, datamodule.test_features, (start_logits, end_logits))\n","\n","    input_df[\"PredictionString\"] = input_df[\"id\"].map(predictions)\n","    input_df[\"PredictionString\"] = postprocess_cleaned_predictions(input_df)\n","    input_df[[\"id\", \"PredictionString\"]].to_csv(\"submission.csv\", index=False)"],"execution_count":null,"outputs":[]}]}