{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"h7PiERPR3JwG"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1uHWvJoM1MB"},"outputs":[],"source":["!mkdir ~/.kaggle\n","!cp /content/drive/MyDrive/Colab/kaggle/kaggle.json ~/.kaggle\n","\n","!pip install --upgrade --force-reinstall --no-deps kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Urj-Rfgn3UBV"},"outputs":[],"source":["!cp -r /content/drive/MyDrive/Colab/kaggle/chaii-hindi-and-tamil-question-answering/input /content\n","!mkdir /content/checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6dGMLhJ3f0r"},"outputs":[],"source":["!pip install -U pytorch-lightning transformers wandb sentencepiece torchsummaryX"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3603,"status":"ok","timestamp":1636384805202,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"EGc_0a3l32o8"},"outputs":[],"source":["import os\n","import random\n","from dataclasses import dataclass\n","import copy\n","import gc\n","\n","import numpy as np\n","import pandas as pd\n","import math\n","import sklearn.model_selection as sms\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n","import torch.optim as optim\n","from torchmetrics import MeanMetric\n","from torch.autograd import Variable\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer, seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n","from pytorch_lightning.loggers import WandbLogger\n","import wandb\n","\n","from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n","from torchsummaryX import summary\n","\n","from collections import defaultdict, OrderedDict\n","from tqdm.notebook import tqdm\n","\n","import json\n","from pathlib import Path\n","from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"markdown","metadata":{"id":"2pbFM7_B4Emp"},"source":["## config"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1636384805203,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"HemZ1ng-4FIW"},"outputs":[],"source":["@dataclass\n","class Config:\n","    #########################\n","    # Globals #\n","    #########################\n","    exp_name = \"exp015\"\n","    debug = False\n","    gpus = 1\n","    num_workers = 8\n","    num_epochs = 2\n","    grad_accumulate = 3\n","    fp16 = False\n","    seed = 1234\n","    #########################\n","    # Data #\n","    #########################\n","    train_csv = \"input/train.csv\"\n","    test_csv = \"input/test.csv\"\n","    sample_submission_csv = \"input/sample_submission.csv\"\n","    mlqa_hindi_csv = \"input/external_data/mlqa_hindi.csv\"\n","    xquad_hindi_csv = \"input/external_data/xquad_hindi.csv\"\n","    xquad_tamil_csv = \"input/external_data/squad_translated_tamil.csv\"\n","    checkpoint_dir = \"drive/MyDrive/Colab/kaggle/chaii-hindi-and-tamil-question-answering/checkpoint\"\n","    #########################\n","    # Split #\n","    #########################\n","    split_name = \"StratifiedKFold\"\n","    split_params = {\n","        \"n_splits\": 5,\n","        \"random_state\": 1234,\n","        \"shuffle\": True,\n","        }\n","    #########################\n","    # Tokenizer #\n","    #########################\n","    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n","    max_seq_length = 400\n","    doc_stride = 135\n","    truncation = \"only_second\"\n","    padding = \"max_length\"\n","    #########################\n","    # Dataset #\n","    #########################\n","    #########################\n","    # DataLoader #\n","    #########################\n","    train_batch_size = 3\n","    valid_batch_size = 8\n","    test_batch_size = 128\n","    #########################\n","    # Model #\n","    #########################\n","    base_model_name = \"deepset/xlm-roberta-large-squad2\"\n","    base_model_config = \"deepset/xlm-roberta-large-squad2\"\n","    num_classes = 2\n","    init_layers = 1\n","    #########################\n","    # Criterion #\n","    #########################\n","    loss_name = \"CrossEntropyLoss\"\n","    loss_params = {\n","        \"ignore_index\": -1,\n","    }\n","    #########################\n","    # Optimizer #\n","    #########################\n","    optimizer_name = \"AdamW\"\n","    optimizer_params = {\n","        \"lr\": 1.5e-5,\n","        \"weight_decay\": 1e-2,\n","        \"eps\": 1e-8,\n","        \"correct_bias\": True\n","    }\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    header_weight_decay = 1e-2\n","    header_lr = 1e-3\n","    lr_decay = 0.98\n","    ######################\n","    # Scheduler #\n","    ######################\n","    scheduler_name = \"linear-warmup\"\n","    scheduler_params = {\n","        \"warmup_ratio\": 0.1,\n","    }\n","    ######################\n","    # Callbacks #\n","    ######################\n","    model_checkpoint_params = {\n","        \"monitor\": \"val/loss_epoch\",\n","        \"save_top_k\": 1,\n","        \"save_weights_only\": True,\n","        \"mode\": \"min\",\n","    }\n","    early_stopping_params = {\n","        \"monitor\": \"val/jaccard_epoch\",\n","        \"min_delta\": 0.0,\n","        \"patience\": 5,\n","        \"verbose\": False,\n","        \"mode\": \"max\",\n","    }\n","    wandb_logger_params = {\n","        \"project\": \"kaggle-chaii-hindi-and-tamil-question-answering\"\n","    }\n","\n","\n","cfg = Config()"]},{"cell_type":"markdown","metadata":{"id":"N75ldQ_N4_lF"},"source":["## utils"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1636384805204,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"N1OPGpKZ48h-"},"outputs":[],"source":["def load_dataset(cfg):\n","    train_df = pd.read_csv(cfg.train_csv, nrows= 100 if cfg.debug else None)\n","    test_df = pd.read_csv(cfg.test_csv)\n","    external_mlqa_df = pd.read_csv(cfg.mlqa_hindi_csv)\n","    external_xquad_df = pd.read_csv(cfg.xquad_hindi_csv)\n","    external_tamil_xquad_df = pd.read_csv(cfg.xquad_tamil_csv)\n","    external_tamil_xquad_df['language'] = \"tamil\"\n","    external_tamil_xquad_df[\"answer_start\"] = external_tamil_xquad_df[\"answer_start\"].astype(int)\n","    external_train_df = pd.concat([\n","                                   external_mlqa_df, \n","                                   external_xquad_df, \n","                                   external_tamil_xquad_df\n","                                   ], axis=0).reset_index(drop=True)\n","    sample_submission_df = pd.read_csv(cfg.sample_submission_csv)\n","\n","    return train_df, test_df, external_train_df, sample_submission_df\n","\n","\n","def get_split(cfg):\n","    split_name = cfg.split_name\n","    split_params = cfg.split_params\n","\n","    return sms.__getattribute__(split_name)(**split_params)\n","\n","\n","def get_fold(cfg, train_df: pd.DataFrame, y_train: pd.DataFrame):\n","    splitter = get_split(cfg)\n","    train_df[\"fold\"] = -1\n","    for fold_id, (train_idx, valid_idx) in enumerate(splitter.split(train_df, y_train)):\n","        train_df.loc[valid_idx, \"fold\"] = int(fold_id)\n","\n","    return train_df\n","\n","\n","def convert_answers(row):\n","    return {\"answer_start\": [row[0]], \"text\": [row[1]]}\n","\n","\n","def jaccard(str1, str2):\n","    a = set(str1.lower().split())\n","    b = set(str2.lower().split())\n","    c = a.intersection(b)\n","\n","    return float(len(c)) / (len(a) + len(b) - len(c))"]},{"cell_type":"markdown","metadata":{"id":"RBsFiLLN5EVr"},"source":["## load data"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":585},"executionInfo":{"elapsed":1359,"status":"ok","timestamp":1636384806557,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"AwmhgEfk4vR8","outputId":"c2761f08-4abc-493d-de0a-f9a00fd0f628"},"outputs":[{"data":{"text/html":["\u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eid\u003c/th\u003e\n","      \u003cth\u003econtext\u003c/th\u003e\n","      \u003cth\u003equestion\u003c/th\u003e\n","      \u003cth\u003eanswer_text\u003c/th\u003e\n","      \u003cth\u003eanswer_start\u003c/th\u003e\n","      \u003cth\u003elanguage\u003c/th\u003e\n","      \u003cth\u003efold\u003c/th\u003e\n","      \u003cth\u003eanswers\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e903deec17\u003c/td\u003e\n","      \u003ctd\u003eஒரு சாதாரண வளர்ந்த மனிதனுடைய எலும்புக்கூடு பின...\u003c/td\u003e\n","      \u003ctd\u003eமனித உடலில் எத்தனை எலும்புகள் உள்ளன?\u003c/td\u003e\n","      \u003ctd\u003e206\u003c/td\u003e\n","      \u003ctd\u003e53\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [53], 'text': ['206']}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003ed9841668c\u003c/td\u003e\n","      \u003ctd\u003eகாளிதாசன் (தேவநாகரி: कालिदास) சமஸ்கிருத இலக்கி...\u003c/td\u003e\n","      \u003ctd\u003eகாளிதாசன் எங்கு பிறந்தார்?\u003c/td\u003e\n","      \u003ctd\u003eகாசுமீரில்\u003c/td\u003e\n","      \u003ctd\u003e2358\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e2\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [2358], 'text': ['காசுமீரில்']}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e29d154b56\u003c/td\u003e\n","      \u003ctd\u003eசர் அலெக்ஸாண்டர் ஃபிளெமிங் (Sir Alexander Flem...\u003c/td\u003e\n","      \u003ctd\u003eபென்சிலின் கண்டுபிடித்தவர் யார்?\u003c/td\u003e\n","      \u003ctd\u003eசர் அலெக்ஸாண்டர் ஃபிளெமிங்\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e3\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [0], 'text': ['சர் அலெக்ஸாண்ட...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e41660850a\u003c/td\u003e\n","      \u003ctd\u003eகுழந்தையின் அழுகையை  நிறுத்தவும், தூங்க வைக்கவ...\u003c/td\u003e\n","      \u003ctd\u003eதமிழ்நாட்டில் குழந்தைகளை தூங்க வைக்க பாடும் பா...\u003c/td\u003e\n","      \u003ctd\u003eதாலாட்டு\u003c/td\u003e\n","      \u003ctd\u003e68\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e1\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [68], 'text': ['தாலாட்டு']}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eb29c82c22\u003c/td\u003e\n","      \u003ctd\u003eசூரியக் குடும்பம் \\nசூரியக் குடும்பம் (Solar S...\u003c/td\u003e\n","      \u003ctd\u003eபூமியின் அருகில் உள்ள விண்மீன் எது?\u003c/td\u003e\n","      \u003ctd\u003eசூரியனும்\u003c/td\u003e\n","      \u003ctd\u003e585\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [585], 'text': ['சூரியனும்']}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e11291\u003c/th\u003e\n","      \u003ctd\u003e10178\u003c/td\u003e\n","      \u003ctd\u003eஎமிட் அரசாங்கத்தின் பணி மற்றும் நோக்கங்களை ஆதர...\u003c/td\u003e\n","      \u003ctd\u003eமாணவர்களின் எண்ணிக்கை என்ன?\u003c/td\u003e\n","      \u003ctd\u003e50\u003c/td\u003e\n","      \u003ctd\u003e421\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e-1\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [421], 'text': ['50']}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e11292\u003c/th\u003e\n","      \u003ctd\u003e10179\u003c/td\u003e\n","      \u003ctd\u003eதஜிகிஸ்தான்(I /tɑːdʒiːkᵻstɑːn/, /tədʒiːkᵻstæn/...\u003c/td\u003e\n","      \u003ctd\u003eதஜிகிஸ்தானில் எத்தனை பேர் கணக்கிடப்படுகிறார்கள்?\u003c/td\u003e\n","      \u003ctd\u003e8 மில்லியன்\u003c/td\u003e\n","      \u003ctd\u003e353\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e-1\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [353], 'text': ['8 மில்லியன்']}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e11293\u003c/th\u003e\n","      \u003ctd\u003e10180\u003c/td\u003e\n","      \u003ctd\u003eடீனேஜர் சஞ்சய மலகார் தனது அசாதாரணமான ஹேர்டோவிற...\u003c/td\u003e\n","      \u003ctd\u003eசஞ்சய மலகார் அமெரிக்க சிலை மீது நீக்கப்பட்டதை ...\u003c/td\u003e\n","      \u003ctd\u003eஏப்ரல் 18\u003c/td\u003e\n","      \u003ctd\u003e143\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e-1\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [143], 'text': ['ஏப்ரல் 18']}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e11294\u003c/th\u003e\n","      \u003ctd\u003e10181\u003c/td\u003e\n","      \u003ctd\u003e\"இஸ்லாமிய தத்துவத்திற்கான\" பொதுவான வரையறைகளில்...\u003c/td\u003e\n","      \u003ctd\u003eஇபின் சினாவால் எத்தனை புத்தகங்கள் எழுதப்பட்டுள...\u003c/td\u003e\n","      \u003ctd\u003e450\u003c/td\u003e\n","      \u003ctd\u003e334\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e-1\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [334], 'text': ['450']}\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e11295\u003c/th\u003e\n","      \u003ctd\u003e10182\u003c/td\u003e\n","      \u003ctd\u003eஅவரது எதிரிகள் ஹேக்கை \"Neoleiberalmismism\" ஒரு...\u003c/td\u003e\n","      \u003ctd\u003eஹெயெக்கின் அரசியலமைப்பை சுதந்திரத்தின் அரசியலம...\u003c/td\u003e\n","      \u003ctd\u003eசாமுவேல் பிரிட்டன்\u003c/td\u003e\n","      \u003ctd\u003e103\u003c/td\u003e\n","      \u003ctd\u003etamil\u003c/td\u003e\n","      \u003ctd\u003e-1\u003c/td\u003e\n","      \u003ctd\u003e{'answer_start': [103], 'text': ['சாமுவேல் பிர...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e11296 rows × 8 columns\u003c/p\u003e\n","\u003c/div\u003e"],"text/plain":["              id  ...                                            answers\n","0      903deec17  ...            {'answer_start': [53], 'text': ['206']}\n","1      d9841668c  ...   {'answer_start': [2358], 'text': ['காசுமீரில்']}\n","2      29d154b56  ...  {'answer_start': [0], 'text': ['சர் அலெக்ஸாண்ட...\n","3      41660850a  ...       {'answer_start': [68], 'text': ['தாலாட்டு']}\n","4      b29c82c22  ...     {'answer_start': [585], 'text': ['சூரியனும்']}\n","...          ...  ...                                                ...\n","11291      10178  ...            {'answer_start': [421], 'text': ['50']}\n","11292      10179  ...   {'answer_start': [353], 'text': ['8 மில்லியன்']}\n","11293      10180  ...     {'answer_start': [143], 'text': ['ஏப்ரல் 18']}\n","11294      10181  ...           {'answer_start': [334], 'text': ['450']}\n","11295      10182  ...  {'answer_start': [103], 'text': ['சாமுவேல் பிர...\n","\n","[11296 rows x 8 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_df, test_df, external_train_df, sample_submission_df = load_dataset(cfg)\n","\n","train_df = get_fold(cfg, train_df, train_df[\"language\"])\n","\n","external_train_df[\"fold\"] = -1\n","external_train_df[\"id\"] = list(np.arange(1, len(external_train_df) + 1))\n","train_df = pd.concat([train_df, external_train_df], axis=0).reset_index(drop=True)\n","\n","# 改行文字の削除\n","# train_df[\"context\"] = train_df[\"context\"].apply(lambda x: \" \".join(x.split()))\n","# train_df[\"question\"] = train_df[\"question\"].apply(lambda x: \" \".join(x.split()))\n","\n","train_df[\"answers\"] = train_df[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)\n","train_df"]},{"cell_type":"markdown","metadata":{"id":"WcJnS8s25JLn"},"source":["## preprocess"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1636384806558,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"nxgeHj2A82w0"},"outputs":[],"source":["def prepare_train_features(cfg, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=cfg.truncation,\n","        max_length=cfg.max_seq_length,\n","        stride=cfg.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=cfg.padding,\n","        # return_tensors=\"pt\"\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids  = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature[\"input_ids\"] = input_ids\n","        feature[\"attention_mask\"] = attention_mask\n","        feature[\"offset_mapping\"] = offsets\n","        feature[\"example_id\"] = example[\"id\"]\n","        feature[\"sequence_ids\"] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id) # cls_token_id = 0, pad_token_id = 1\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        # sample_indx = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0: # sampleにanswerがなければ開始・終了位置をCLS(=[CLS], \u003cs\u003e)にする\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0] # answer部分の文字レベルの開始位置\n","            end_char = start_char + len(answers[\"text\"][0]) # answer部分の文字レベルの終了位置\n","\n","            token_start_index = 0 # sequenceにおけるcontext部分のトークンレベルの開始位置\n","            while sequence_ids[token_start_index] != 1: # sequence_idsが1(=context部分)になるまで足す(\u003cs\u003e,\u003c/s\u003e, querstion部分を飛ばすイメージ)\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1 # sequenceにおけるcontext部分のトークンレベルの終了位置\n","            while sequence_ids[token_end_index] != 1: # sequence_idsが1(=context部分)になるまで引く(paddingはNoneなので\u003cpad\u003e部分を引くイメージ)\n","                token_end_index -= 1\n","\n","            # token_start_index, token_end_indexがanswer部分の範囲外にあればpositionを0(=[CLS], \u003cs\u003e)とする(truncationされてる場合はこっち)\n","            if not (offsets[token_start_index][0] \u003c= start_char and offsets[token_end_index][1] \u003e= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            # star_char, end_charを含むtokenまでtoken_start_index, token_end_indexを調整\n","            else:\n","                while token_start_index \u003c len(offsets) and offsets[token_start_index][0] \u003c= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1 # 行き過ぎた1token分戻す\n","\n","                while offsets[token_end_index][1] \u003e= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1 # 行き過ぎた1token分戻す\n","\n","        features.append(feature)\n","\n","    return features\n","\n","\n","def prepare_test_features(cfg, example, tokenizer):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=cfg.truncation,\n","        max_length=cfg.max_seq_length,\n","        stride=cfg.doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=cfg.padding,\n","        # return_tensors=\"pt\"\n","    )\n","\n","    features = []\n","    for i in range(len(tokenized_example[\"input_ids\"])):\n","        feature = {}\n","        feature[\"example_id\"] = example[\"id\"]\n","        feature[\"context\"] = example[\"context\"]\n","        feature[\"question\"] = example[\"question\"]\n","        feature[\"input_ids\"] = tokenized_example[\"input_ids\"][i]\n","        feature[\"attention_mask\"] = tokenized_example[\"attention_mask\"][i]\n","        feature[\"offset_mapping\"] = tokenized_example[\"offset_mapping\"][i]\n","        feature[\"sequence_ids\"] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n","        features.append(feature)\n","\n","    return features\n","\n","\n","def postprocess_qa_predictions(tokenizer, examples: pd.DataFrame, features, raw_predictions, n_best_size=20, max_answer_length=30):\n","    '''予測値の後処理関数\n","    '''\n","    all_start_logits, all_end_logits = raw_predictions\n","\n","    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])} # dataframeのidをkeyとしたindexのmapping用dict\n","    features_per_example = defaultdict(list) # dataframeのidに対応するexample_id_to_indexのidをkeyとしたdict。valueはfeaturesに対応するidのリスト\n","    for i, feature in enumerate(features):\n","        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","    predictions = OrderedDict()\n","    # print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n","\n","    for example_index, example in examples.iterrows():\n","        feature_indices = features_per_example[example_index] # 対応するfeaturesのindexを取り出す\n","\n","        # min_null_score = None\n","        valid_answers = []\n","\n","        context = example[\"context\"]\n","        for feature_index in feature_indices:\n","            start_logits = all_start_logits[feature_index]\n","            end_logits = all_end_logits[feature_index]\n","\n","            sequence_ids = features[feature_index][\"sequence_ids\"]\n","            context_index = 1 # contextのsequence_id(questionは0)\n","            \n","            # contextのみoffset_mappingを保持(questionのoffset_mappingをNoneに)\n","            features[feature_index][\"offset_mapping\"] = [\n","                (o if sequence_ids[k] == context_index else None)\n","                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n","            ]\n","            offset_mapping = features[feature_index][\"offset_mapping\"]\n","            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n","\n","            # feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n","            # if min_null_score is None or min_null_score \u003c feature_null_score:\n","            #     min_null_score = feature_null_score\n","\n","            # start_indexes = np.argsort(start_logits)[-1: -n_best_size - 1: -1].tolist()\n","            # end_indexes = np.argsort(end_logits)[-1: -n_best_size - 1: -1].tolist()\n","            start_indexes = np.argsort(start_logits)[::-1][:n_best_size].tolist()\n","            end_indexes = np.argsort(end_logits)[::-1][:n_best_size].tolist()\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    if (\n","                        start_index \u003e= len(offset_mapping)\n","                        or end_index \u003e= len(offset_mapping)\n","                        or offset_mapping[start_index] is None\n","                        or offset_mapping[end_index] is None\n","                    ):\n","                        continue\n","                    # Don't consider answers with a length that is either \u003c 0 or \u003e max_answer_length.\n","                    if end_index \u003c start_index or end_index - start_index + 1 \u003e max_answer_length:\n","                        continue\n","\n","                    start_char = offset_mapping[start_index][0]\n","                    end_char = offset_mapping[end_index][1]\n","                    valid_answers.append(\n","                        {\n","                            \"score\": start_logits[start_index] + end_logits[end_index],\n","                            \"text\": context[start_char: end_char]\n","                        }\n","                    )\n","        # 各レコード・チャンク(feature)におけるstart+end出力値のスコアが最も大きいペアを最終的な予測値とする\n","        if len(valid_answers) \u003e 0:\n","            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n","        else:\n","            best_answer = {\"text\": \"\", \"score\": 0.0}\n","\n","        predictions[example[\"id\"]] = best_answer[\"text\"]\n","\n","    return predictions\n","\n","\n","def postprocess_cleaned_predictions(input_df: pd.DataFrame):\n","    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"–\",  \",\", \";\"]\n","    bad_endings = [\"...\", \"-\", \"(\", \")\", \"–\", \",\", \";\"]\n","\n","    tamil_ad = \"கி.பி\"\n","    tamil_bc = \"கி.மு\"\n","    tamil_km = \"கி.மீ\"\n","    hindi_ad = \"ई\"\n","    hindi_bc = \"ई.पू\"\n","\n","\n","    cleaned_preds = []\n","    for pred, context in input_df[[\"PredictionString\", \"context\"]].to_numpy():\n","        if pred == \"\":\n","            cleaned_preds.append(pred)\n","            continue\n","        while any([pred.startswith(y) for y in bad_starts]):\n","            pred = pred[1:]\n","        while any([pred.endswith(y) for y in bad_endings]):\n","            if pred.endswith(\"...\"):\n","                pred = pred[:-3]\n","            else:\n","                pred = pred[:-1]\n","        if pred.endswith(\"...\"):\n","                pred = pred[:-3]\n","\n","        if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n","            pred = pred+\".\"\n","\n","        cleaned_preds.append(pred)\n","        \n","    return cleaned_preds"]},{"cell_type":"markdown","metadata":{"id":"3nn0RX0BicrZ"},"source":["## Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1636384806558,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"OHSxITfLa9y8"},"outputs":[],"source":["class ChaiiDataset(Dataset):\n","    def __init__(self, features, phase):\n","        super(ChaiiDataset, self).__init__()\n","        self.features = features\n","        self.phase = phase\n","        \n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        feature = self.features[index]\n","        segment_target = torch.zeros(400, dtype=torch.long)\n","        segment_target[feature[\"start_position\"]: feature[\"end_position\"] + 1] = 1\n","\n","        if self.phase == \"train\":\n","            return {\n","                \"input_ids\": torch.tensor(feature[\"input_ids\"], dtype=torch.long),\n","                \"attention_mask\": torch.tensor(feature[\"attention_mask\"], dtype=torch.long),\n","                \"offset_mapping\": torch.tensor(feature[\"offset_mapping\"], dtype=torch.long),\n","                \"start_position\": torch.tensor(feature[\"start_position\"], dtype=torch.long),\n","                \"end_position\": torch.tensor(feature[\"end_position\"], dtype=torch.long),\n","                \"segment_target\": segment_target,\n","            }\n","        else:\n","            return {\n","                \"input_ids\": torch.tensor(feature[\"input_ids\"], dtype=torch.long),\n","                \"attention_mask\": torch.tensor(feature[\"attention_mask\"], dtype=torch.long),\n","                \"offset_mapping\": torch.tensor(feature[\"offset_mapping\"], dtype=torch.long),\n","                \"sequence_ids\": feature[\"sequence_ids\"],\n","                \"id\": feature[\"example_id\"],\n","                \"context\": feature[\"context\"],\n","                \"question\": feature[\"question\"],\n","            }\n","\n","\n","class ChaiiDataModule(pl.LightningDataModule):\n","    def __init__(self, cfg, tokenizer, input_df: pd.DataFrame, phase: str, fold: int = 0):\n","        super(ChaiiDataModule, self).__init__()\n","        self.cfg = cfg\n","        self.tokenizer = tokenizer\n","        self.input_df = input_df\n","        self.phase = phase\n","        self.fold = fold\n","\n","    def get_train_features(self, cfg, input_df, tokenizer):\n","        features = []\n","        for i, row in tqdm(input_df.iterrows(), total=len(input_df), desc=\"[get features]\"):\n","            features += prepare_train_features(cfg, row, tokenizer)\n","        \n","        return features\n","\n","    def get_test_features(self, cfg, input_df, tokenizer):\n","        features = []\n","        for i, row in tqdm(input_df.iterrows(), total=len(input_df), desc=\"[get features]\"):\n","            features += prepare_test_features(cfg, row, tokenizer)\n","        \n","        return features\n","\n","    def setup(self, stage=None):\n","        assert self.phase in (\"train\", \"test\"), \"Input phase is not exist.\"\n","        if self.phase == \"train\":\n","            self.train_df = self.input_df[self.input_df[\"fold\"] != self.fold].reset_index(drop=True)\n","            self.valid_df = self.input_df[self.input_df[\"fold\"] == self.fold].reset_index(drop=True)\n","\n","            self.train_features = self.get_train_features(self.cfg, self.train_df, self.tokenizer)\n","            self.valid_features = self.get_train_features(self.cfg, self.valid_df, self.tokenizer)\n","\n","            self.train_dataset = ChaiiDataset(self.train_features, self.phase)\n","            self.valid_dataset = ChaiiDataset(self.valid_features, self.phase)\n","            print(f\"Number of train features: {len(self.train_dataset)}, Number of valid features: {len(self.valid_dataset)}\")\n","        elif self.phase == \"test\":\n","            self.test_features = self.get_test_features(self.cfg, self.input_df, self.tokenizer)\n","            self.test_dataset = ChaiiDataset(self.test_features, self.phase)\n","            print(f\"Number of test features: {len(self.test_dataset)}\")\n","        else:\n","            raise NotImplementedError\n","        \n","    def train_dataloader(self):\n","        train_sampler = RandomSampler(self.train_dataset)\n","        return DataLoader(self.train_dataset, batch_size=self.cfg.train_batch_size, num_workers=self.cfg.num_workers, pin_memory=True, drop_last=False, sampler=train_sampler)\n","\n","    def val_dataloader(self):\n","        valid_sampler = SequentialSampler(self.valid_dataset)\n","        return DataLoader(self.valid_dataset, batch_size=self.cfg.valid_batch_size, num_workers=self.cfg.num_workers, pin_memory=True, drop_last=False, sampler=valid_sampler)\n","\n","    def predict_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=self.cfg.test_batch_size, num_workers=self.cfg.num_workers, pin_memory=True, shuffle=False, drop_last=False)"]},{"cell_type":"markdown","metadata":{"id":"HhzSnmitfua7"},"source":["## Model"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1636384806559,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"IcStqJUDfees"},"outputs":[],"source":["class ChaiiModel(nn.Module):\n","    def __init__(self, cfg):\n","        super(ChaiiModel, self).__init__()\n","        self.cfg = cfg\n","        self.model_config = AutoConfig.from_pretrained(self.cfg.base_model_config)\n","        self.encoder = AutoModel.from_pretrained(self.cfg.base_model_name, config=self.model_config)\n","        self.classifier = nn.Linear(self.model_config.hidden_size, cfg.num_classes)\n","        self._init_header_weights(self.classifier)\n","        # self._init_roberta_weights(self.encoder)\n","\n","    def _init_header_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def _init_roberta_weights(self, model):\n","        for layer in model.encoder.layer[-cfg.init_layers:]:\n","            for module in layer.modules():\n","                if isinstance(module, nn.Linear):\n","                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n","                    if module.bias is not None:\n","                        module.bias.data.zero_()\n","                elif isinstance(module, nn.Embedding):\n","                    module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n","                    if module.padding_idx is not None:\n","                        module.weight.data[module.padding_idx].zero_()\n","                elif isinstance(module, nn.LayerNorm):\n","                    module.bias.data.zero_()\n","                    module.weight.data.fill_(1.0)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        output = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        x = self.classifier(output[0]) # (batch_size, hidden_size, num_classes)\n","        x0, x1 = x.split(1, dim=-1) # (batch_size, hidden_size, 1)\n","        start_logits, end_logits = x0.squeeze(-1), x1.squeeze(-1) # (batch_size, hidden_size)\n","\n","        return start_logits, end_logits\n","\n","\n","# model = ChaiiModel(cfg)\n","# summary(model, torch.zeros(1, 400, dtype=torch.long))"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1636384806559,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"f2v58Py062GP"},"outputs":[],"source":["def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    p = len(gt_sorted)\n","    gts = gt_sorted.sum()\n","    intersection = gts - gt_sorted.float().cumsum(0)\n","    union = gts + (1 - gt_sorted).float().cumsum(0)\n","    jaccard = 1. - intersection / union\n","    if p \u003e 1: # cover 1-pixel case\n","        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n","    return jaccard\n","\n","# --------------------------- BINARY LOSSES ---------------------------\n","def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n","                          for log, lab in zip(logits, labels))\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","    if len(labels) == 0:\n","        # only void pixels, the gradients should be 0\n","        return logits.sum() * 0.\n","    signs = 2. * labels.float() - 1.\n","    errors = (1. - logits * Variable(signs))\n","    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n","    perm = perm.data\n","    gt_sorted = labels[perm]\n","    grad = lovasz_grad(gt_sorted)\n","    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = scores.view(-1)\n","    labels = labels.view(-1)\n","    if ignore is None:\n","        return scores, labels\n","    valid = (labels != ignore)\n","    vscores = scores[valid]\n","    vlabels = labels[valid]\n","    return vscores, vlabels\n","\n","\n","class StableBCELoss(torch.nn.modules.Module):\n","    def __init__(self):\n","         super(StableBCELoss, self).__init__()\n","    def forward(self, input, target):\n","         neg_abs = - input.abs()\n","         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n","         return loss.mean()\n","\n","\n","def binary_xloss(logits, labels, ignore=None):\n","    \"\"\"\n","    Binary Cross entropy loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      ignore: void class id\n","    \"\"\"\n","    logits, labels = flatten_binary_scores(logits, labels, ignore)\n","    loss = StableBCELoss()(logits, Variable(labels.float()))\n","    return loss\n","\n","\n","# --------------------------- HELPER FUNCTIONS ---------------------------\n","def isnan(x):\n","    return x != x\n","    \n","    \n","def mean(l, ignore_nan=False, empty=0):\n","    \"\"\"\n","    nanmean compatible with generators.\n","    \"\"\"\n","    l = iter(l)\n","    if ignore_nan:\n","        l = ifilterfalse(isnan, l)\n","    try:\n","        n = 1\n","        acc = next(l)\n","    except StopIteration:\n","        if empty == 'raise':\n","            raise ValueError('Empty mean')\n","        return empty\n","    for n, v in enumerate(l, 2):\n","        acc += v\n","    if n == 1:\n","        return acc\n","    return acc / n\n","\n","\n","class LovaszHingeLoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(LovaszHingeLoss, self).__init__()\n","\n","    def forward(self, inputs, targets):\n","        # inputs = torch.sigmoid(inputs)\n","        Lovasz = lovasz_hinge(inputs, targets, per_image=True)\n","\n","        return Lovasz"]},{"cell_type":"markdown","metadata":{"id":"lZ15q-gb5DiO"},"source":["## training"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":604,"status":"ok","timestamp":1636384807157,"user":{"displayName":"永友遥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8XdDU-0vxRr4CR7yDupYRXQbKwd1ubt2GOx9uTQ=s64","userId":"11743586908271963047"},"user_tz":-540},"id":"8mKGz0xAqy3L"},"outputs":[],"source":["def get_optimizer_grouped_parameters(cfg, model):\n","    no_decay = cfg.no_decay\n","    # header layerのweight_decay, lr\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if \"classifier\" in n],\n","            \"weight_decay\": cfg.header_weight_decay,\n","            \"lr\": cfg.header_lr\n","        },\n","    ]\n","\n","    # num_layers = model.base_model.config.num_hidden_layers\n","    layers = [getattr(model, \"base_model\").embeddings] + list(getattr(model, \"base_model\").encoder.layer)\n","    layers.reverse()\n","    lr = cfg.optimizer_params[\"lr\"]\n","    \n","    for layer in layers:\n","        lr *= cfg.lr_decay\n","        optimizer_grouped_parameters += [\n","            # no_decayのリストに含まれないパラメータはweight decayを設定\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": cfg.optimizer_params[\"weight_decay\"],\n","                \"lr\": lr,\n","            },\n","            # no_decayのリストに含まれるパラメータはweight decayを設定しない\n","            {\n","                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","                \"lr\": lr,\n","            }\n","        ]\n","    \n","    return optimizer_grouped_parameters\n","\n","\n","def get_criterion(cfg):\n","    loss_name = cfg.loss_name\n","    loss_params = cfg.loss_params\n","    return nn.__getattribute__(loss_name)(**loss_params)\n","\n","\n","def get_optimizer(cfg, model=None, optimizer_grouped_parameters=None):\n","    optimizer_name = cfg.optimizer_name\n","    optimizer_params = cfg.optimizer_params\n","\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": cfg.optimizer_params[\"weight_decay\"],\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","\n","    if optimizer_name == \"AdamW\":\n","        return AdamW(\n","            optimizer_grouped_parameters,\n","            **optimizer_params\n","        )\n","    else:\n","        return optim.__getattribute__(optimizer_name)(model.parameters(), **optimizer_params)\n","\n","\n","def get_scheduler(cfg, optimizer, num_warmup_steps=None, num_training_steps=None):\n","    scheduler_name = cfg.scheduler_name\n","    scheduler_params = cfg.scheduler_params\n","\n","    if scheduler_name == \"cosine-warmup\":\n","        return get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","        )\n","    elif scheduler_name == \"linear-warmup\":\n","        return get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","        )\n","    # elif scheduler_name not in (\"cosine-warmup\", \"linear-warmup\"):\n","    #     return optim.lr_scheduler.__getattribute__(scheduler_name)(optimizer, **scheduler_params)\n","\n","\n","class ChaiiLightningModule(pl.LightningModule):\n","    def __init__(self, cfg):\n","        super(ChaiiLightningModule, self).__init__()\n","        self.cfg = cfg\n","        self.model = ChaiiModel(self.cfg)\n","        self.start_criterion = get_criterion(self.cfg)\n","        self.end_criterion = get_criterion(self.cfg)\n","        self.train_mean_metric1 = MeanMetric()\n","        self.train_mean_metric2 = MeanMetric()\n","        self.valid_mean_metric1 = MeanMetric()\n","        self.valid_mean_metric2 = MeanMetric()\n","        self.lovasz_hinge = LovaszHingeLoss()\n","\n","    def forward(self, input_ids, attention_mask):\n","        output_start, output_end = self.model(input_ids, attention_mask)\n","        return output_start, output_end\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask, target_start, target_end, segment_target = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"start_position\"], batch[\"end_position\"], batch[\"segment_target\"]\n","        output_start, output_end = self.forward(input_ids, attention_mask)\n","\n","        # CrossEntropyLoss\n","        loss_start = self.start_criterion(output_start, target_start)\n","        loss_end = self.end_criterion(output_end, target_end)\n","        bce_loss = (loss_start + loss_end) / 2\n","\n","        # Lovasz-hingeLoss\n","        cum_start_prob = torch.cumsum(torch.sigmoid(output_start), axis=1)\n","        cum_end_prob = torch.fliplr(torch.cumsum(torch.fliplr(torch.sigmoid(output_end)), axis=1))\n","        pred_prob = cum_start_prob * cum_end_prob\n","        lovaszloss = self.lovasz_hinge(pred_prob, segment_target)\n","\n","        loss = bce_loss + lovaszloss * 0.5\n","\n","        self.train_mean_metric1.update(loss)\n","        loss_avg = self.train_mean_metric1.compute()\n","\n","        self.train_mean_metric2.update(bce_loss)\n","        bce_avg = self.train_mean_metric2.compute()\n","\n","        self.log(\"train/loss\", loss_avg, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"train/bce_loss\", bce_avg, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"train/loss_start_epoch\", loss_start.item(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"train/loss_end_epoch\", loss_end.item(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"train/lovasz_hinge_epoch\", lovaszloss.item(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids, attention_mask, target_start, target_end, segment_target = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"start_position\"], batch[\"end_position\"], batch[\"segment_target\"]\n","        output_start, output_end = self.forward(input_ids, attention_mask)\n","\n","        # CrossEntropyLoss\n","        loss_start = self.start_criterion(output_start, target_start)\n","        loss_end = self.end_criterion(output_end, target_end)\n","        bce_loss = (loss_start + loss_end) / 2\n","\n","        # Lovasz-hingeLoss\n","        cum_start_prob = torch.cumsum(torch.sigmoid(output_start), axis=1)\n","        cum_end_prob = torch.fliplr(torch.cumsum(torch.fliplr(torch.sigmoid(output_end)), axis=1))\n","        pred_prob = cum_start_prob * cum_end_prob\n","        lovaszloss = self.lovasz_hinge(pred_prob, segment_target)\n","\n","        loss = bce_loss + lovaszloss * 0.5\n","\n","        self.valid_mean_metric1.update(loss)\n","        loss_avg = self.valid_mean_metric1.compute()\n","\n","        self.valid_mean_metric2.update(bce_loss)\n","        bce_avg = self.valid_mean_metric2.compute()\n","\n","        self.log(\"val/loss\", loss_avg, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"val/bce_loss\", bce_avg, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"val/loss_start_epoch\", loss_start.item(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"val/loss_end_epoch\", loss_end.item(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"val/lovasz_hinge_epoch\", lovaszloss.item(), on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","        outputs = OrderedDict({\n","            \"start_logits\": output_start.detach(),\n","            \"end_logits\": output_end.detach(),\n","        }\n","        )\n","\n","        return outputs\n","\n","    def validation_epoch_end(self, outputs):\n","        pred_start = torch.cat([output[\"start_logits\"] for output in outputs]).cpu().numpy()\n","        pred_end = torch.cat([output[\"end_logits\"] for output in outputs]).cpu().numpy()\n","\n","        preds = postprocess_qa_predictions(\n","            self.trainer.datamodule.tokenizer,\n","            self.trainer.datamodule.valid_df,\n","            copy.deepcopy(self.trainer.datamodule.valid_features[:pred_start.shape[0]]),\n","            (pred_start, pred_end)\n","        )\n","        jaccard_score = np.mean([jaccard(x, y) for x, y in zip(self.trainer.datamodule.valid_df[\"answer_text\"].values, preds.values())], axis=0)\n","\n","        self.log(\"val/jaccard_epoch\", jaccard_score, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","\n","    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n","        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n","        output_start, output_end = self.forward(input_ids, attention_mask)\n","\n","        outputs = OrderedDict({\n","            \"start_logits\": output_start,\n","            \"end_logits\": output_end,\n","        }\n","        )\n","\n","        return outputs\n","\n","    def configure_optimizers(self):\n","        # optimizer_grouped_parameters = get_optimizer_grouped_parameters(self.cfg, self.model)\n","        optimizer = get_optimizer(\n","            self.cfg,\n","            self.model,\n","            # optimizer_grouped_parameters=optimizer_grouped_parameters\n","        )\n","\n","        num_training_steps = math.ceil(len(self.trainer.datamodule.train_dataloader()) / self.trainer.accumulate_grad_batches) * self.trainer.max_epochs\n","        # num_training_steps = len(self.trainer.datamodule.train_dataloader()) * self.trainer.max_epochs\n","        if self.cfg.scheduler_params[\"warmup_ratio\"] \u003e 0:\n","            num_warmup_steps = int(num_training_steps * self.cfg.scheduler_params[\"warmup_ratio\"])\n","        else:\n","            num_warmup_steps = 0\n","        scheduler = get_scheduler(self.cfg, optimizer, num_warmup_steps, num_training_steps)\n","        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n","        print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n","\n","        return [optimizer], [scheduler]\n","\n","\n","def run_fold(cfg, train_df, fold, tokenizer):\n","    seed_everything(cfg.seed)\n","    checkpoint_path = cfg.checkpoint_dir + \"/\" + cfg.exp_name\n","\n","    if not os.path.exists(checkpoint_path):\n","        os.makedirs(checkpoint_path)\n","\n","    checkpoint_callback = ModelCheckpoint(\n","        dirpath=checkpoint_path,\n","        filename=f\"{cfg.exp_name}-fold-{fold}\" + \"-{epoch}\",\n","        **cfg.model_checkpoint_params,\n","    )\n","\n","    early_stopping_callback = EarlyStopping(**cfg.early_stopping_params)\n","\n","    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n","\n","    wandb_logger = WandbLogger(\n","        name=f\"{cfg.exp_name}_fold_{fold}\",\n","        **cfg.wandb_logger_params,\n","    )\n","\n","    trainer = Trainer(\n","        default_root_dir=cfg.checkpoint_dir,\n","        gpus=cfg.gpus,\n","        max_epochs=cfg.num_epochs,\n","        accumulate_grad_batches=cfg.grad_accumulate,\n","        precision=16 if cfg.fp16 else 32,\n","        callbacks=[\n","            checkpoint_callback,\n","            # early_stopping_callback,\n","            lr_monitor,\n","        ],\n","        logger=[\n","            wandb_logger,\n","        ],\n","        log_every_n_steps=10,\n","    )\n","\n","    model = ChaiiLightningModule(cfg)\n","    datamodule = ChaiiDataModule(cfg, tokenizer=tokenizer, input_df=train_df, phase=\"train\", fold=fold)\n","    trainer.fit(model, datamodule=datamodule)\n","\n","    wandb.finish()\n","\n","    del trainer, model, datamodule\n","    gc.collect()\n","\n","    return checkpoint_callback.best_model_path, checkpoint_callback.best_model_score.item()\n","\n","\n","def run_training(cfg, train_df):\n","    tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n","\n","    checkpoint_path_list = []\n","    oof_score = 0\n","\n","    for fold_id in range(cfg.split_params[\"n_splits\"]):\n","        checkpoint_path, best_score = run_fold(cfg, train_df, fold_id, tokenizer)\n","        checkpoint_path_list.append(checkpoint_path)\n","        oof_score += best_score / int(cfg.split_params[\"n_splits\"])\n","\n","    print(\"CV jaccard score :\", oof_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":674},"id":"hhGIfEKXBatS"},"outputs":[{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"393231648825485f9aae19c905f1c46b","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/11073 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfb556ab3dcd4a90a646f8569f29626c","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/223 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Number of train features: 22982, Number of valid features: 3106\n","Total Training Steps: 5108, Total Warmup Steps: 510\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mazupero\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/html":["\n","                    Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/mscvibxz\" target=\"_blank\"\u003eexp015_fold_0\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\"\u003edocs\u003c/a\u003e).\u003cbr/\u003e\n","\n","                "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","  | Name               | Type             | Params\n","--------------------------------------------------------\n","0 | model              | ChaiiModel       | 559 M \n","1 | start_criterion    | CrossEntropyLoss | 0     \n","2 | end_criterion      | CrossEntropyLoss | 0     \n","3 | train_mean_metric1 | MeanMetric       | 0     \n","4 | train_mean_metric2 | MeanMetric       | 0     \n","5 | valid_mean_metric1 | MeanMetric       | 0     \n","6 | valid_mean_metric2 | MeanMetric       | 0     \n","7 | lovasz_hinge       | LovaszHingeLoss  | 0     \n","--------------------------------------------------------\n","559 M     Trainable params\n","0         Non-trainable params\n","559 M     Total params\n","2,239.570 Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee5352ce6f294519896e841b0d4d1d5b","version_major":2,"version_minor":0},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd1d77016c644afc990dcc69e8f4f8f7","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71220e8f909f4e499b3750bec1f61847","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"217e4eff7dd748b69246d5c6265d5653","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cbr/\u003eWaiting for W\u0026B process to finish, PID 2066... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59a7e7916079447ab03414243850b8e2","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cstyle\u003e\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    \u003c/style\u003e\n","\u003cdiv class=\"wandb-row\"\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun history:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e███▇▇▆▆▆▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e▁▁▂▂▂▂▃▃▃▃▄▄▄▄▁▁▁▁▁▁▅▅▅▆▆▆▆▇▇▇▇███▂▂▂▂▂▂\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e█▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e▁█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun summary:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e1\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e2.33904\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e2.00088\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e1.26012\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e53.82728\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e1.06947\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e39.34948\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.0902\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e5107\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e0.32429\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e0.29735\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e0.68972\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e0.26628\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e21.67124\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e0.24445\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e15.86461\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.04456\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n","\u003c/div\u003e\u003c/div\u003e\n","Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u003cbr/\u003eSynced \u003cstrong style=\"color:#cdcd00\"\u003eexp015_fold_0\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/mscvibxz\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/mscvibxz\u003c/a\u003e\u003cbr/\u003e\n","Find logs at: \u003ccode\u003e./wandb/run-20211108_152334-mscvibxz/logs\u003c/code\u003e\u003cbr/\u003e\n"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0943b95cb1dd45ea8c60dbeb412332a6","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/11073 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf59d0281b074192a03b4a84eb2f25bf","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/223 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Number of train features: 23279, Number of valid features: 2809\n","Total Training Steps: 5174, Total Warmup Steps: 517\n"]},{"data":{"text/html":["\n","                    Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/26116i8n\" target=\"_blank\"\u003eexp015_fold_1\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\"\u003edocs\u003c/a\u003e).\u003cbr/\u003e\n","\n","                "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","  | Name               | Type             | Params\n","--------------------------------------------------------\n","0 | model              | ChaiiModel       | 559 M \n","1 | start_criterion    | CrossEntropyLoss | 0     \n","2 | end_criterion      | CrossEntropyLoss | 0     \n","3 | train_mean_metric1 | MeanMetric       | 0     \n","4 | train_mean_metric2 | MeanMetric       | 0     \n","5 | valid_mean_metric1 | MeanMetric       | 0     \n","6 | valid_mean_metric2 | MeanMetric       | 0     \n","7 | lovasz_hinge       | LovaszHingeLoss  | 0     \n","--------------------------------------------------------\n","559 M     Trainable params\n","0         Non-trainable params\n","559 M     Total params\n","2,239.570 Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab/kaggle/chaii-hindi-and-tamil-question-answering/checkpoint/exp015 exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f64e45302d34958871b36d7e2476f42","version_major":2,"version_minor":0},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad1c986122824061be6601ecd41b6ee3","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a8c3b3064594e8cb824d91d897de2ca","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0135320d88c4c20aa15950064ac8924","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cbr/\u003eWaiting for W\u0026B process to finish, PID 2521... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70fce9b511dd490a8a5099f052a1078b","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cstyle\u003e\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    \u003c/style\u003e\n","\u003cdiv class=\"wandb-row\"\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun history:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e███▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▁▁▁▁▁▅▅▅▅▆▆▆▆▇▇▇▇▇██▂▂▂▂▂\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e█▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e▁█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun summary:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e1\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e2.3095\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e1.97595\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e1.24808\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e53.86238\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e1.06224\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e39.34191\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.0904\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e5173\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e0.37782\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e0.35532\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e0.64764\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e0.3197\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e23.39514\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e0.29471\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e17.13008\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.04267\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n","\u003c/div\u003e\u003c/div\u003e\n","Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u003cbr/\u003eSynced \u003cstrong style=\"color:#cdcd00\"\u003eexp015_fold_1\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/26116i8n\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/26116i8n\u003c/a\u003e\u003cbr/\u003e\n","Find logs at: \u003ccode\u003e./wandb/run-20211108_163459-26116i8n/logs\u003c/code\u003e\u003cbr/\u003e\n"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc13ef20646a4f6d9a45c67498ca9e8a","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/11073 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0cf074394c20433ab5e5a8b271d9bc9f","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/223 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Number of train features: 23409, Number of valid features: 2679\n","Total Training Steps: 5202, Total Warmup Steps: 520\n"]},{"data":{"text/html":["\n","                    Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/1rdid97b\" target=\"_blank\"\u003eexp015_fold_2\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\"\u003edocs\u003c/a\u003e).\u003cbr/\u003e\n","\n","                "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","  | Name               | Type             | Params\n","--------------------------------------------------------\n","0 | model              | ChaiiModel       | 559 M \n","1 | start_criterion    | CrossEntropyLoss | 0     \n","2 | end_criterion      | CrossEntropyLoss | 0     \n","3 | train_mean_metric1 | MeanMetric       | 0     \n","4 | train_mean_metric2 | MeanMetric       | 0     \n","5 | valid_mean_metric1 | MeanMetric       | 0     \n","6 | valid_mean_metric2 | MeanMetric       | 0     \n","7 | lovasz_hinge       | LovaszHingeLoss  | 0     \n","--------------------------------------------------------\n","559 M     Trainable params\n","0         Non-trainable params\n","559 M     Total params\n","2,239.570 Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab/kaggle/chaii-hindi-and-tamil-question-answering/checkpoint/exp015 exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f33d5cab497415db950040c73fa844f","version_major":2,"version_minor":0},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ca4b0b337554bf2aa07ca6e36d391fd","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"105d7455178142e98b93cda631cdd380","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"270e6a5ca3504138af030e21f4a582cd","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cbr/\u003eWaiting for W\u0026B process to finish, PID 2756... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7d3884b4cea4a73b3339486e85b3c9e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cstyle\u003e\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    \u003c/style\u003e\n","\u003cdiv class=\"wandb-row\"\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun history:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e████▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▁▁▁▁▁▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▂▂▂▂\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e█▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e▁█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun summary:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e1\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e2.28323\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e1.95851\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e1.24155\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e53.53659\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e1.05054\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e39.09648\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.0869\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e5201\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e0.42425\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e0.39904\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e0.65018\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e0.35641\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e25.34958\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e0.33025\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e18.56492\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.04512\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n","\u003c/div\u003e\u003c/div\u003e\n","Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u003cbr/\u003eSynced \u003cstrong style=\"color:#cdcd00\"\u003eexp015_fold_2\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/1rdid97b\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/1rdid97b\u003c/a\u003e\u003cbr/\u003e\n","Find logs at: \u003ccode\u003e./wandb/run-20211108_174724-1rdid97b/logs\u003c/code\u003e\u003cbr/\u003e\n"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df2769a84d564c748bc258190261f9fa","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/11073 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"107b9c4b92e040329515a28d8ddc186d","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/223 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Number of train features: 23156, Number of valid features: 2932\n","Total Training Steps: 5146, Total Warmup Steps: 514\n"]},{"data":{"text/html":["\n","                    Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/3m68g9n4\" target=\"_blank\"\u003eexp015_fold_3\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\"\u003edocs\u003c/a\u003e).\u003cbr/\u003e\n","\n","                "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","  | Name               | Type             | Params\n","--------------------------------------------------------\n","0 | model              | ChaiiModel       | 559 M \n","1 | start_criterion    | CrossEntropyLoss | 0     \n","2 | end_criterion      | CrossEntropyLoss | 0     \n","3 | train_mean_metric1 | MeanMetric       | 0     \n","4 | train_mean_metric2 | MeanMetric       | 0     \n","5 | valid_mean_metric1 | MeanMetric       | 0     \n","6 | valid_mean_metric2 | MeanMetric       | 0     \n","7 | lovasz_hinge       | LovaszHingeLoss  | 0     \n","--------------------------------------------------------\n","559 M     Trainable params\n","0         Non-trainable params\n","559 M     Total params\n","2,239.570 Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab/kaggle/chaii-hindi-and-tamil-question-answering/checkpoint/exp015 exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b71e2734eb6742dcb4ed4367f1ed7d4e","version_major":2,"version_minor":0},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d875e9334e0e403787108d1ce11d3243","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4960ed09e1114d99af0edb7f51a02c78","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2172a44fb9be4046bdfd77fcb9e80a70","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cbr/\u003eWaiting for W\u0026B process to finish, PID 2985... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56789168e47145d18dbb5a3852d244ed","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cstyle\u003e\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    \u003c/style\u003e\n","\u003cdiv class=\"wandb-row\"\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun history:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e████▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▁▁▁▁▁▅▅▅▅▆▆▆▆▇▇▇▇███▂▂▂▂▂\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e█▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun summary:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e1\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e2.31663\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e1.98601\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e1.25172\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e53.6848\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e1.06698\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e39.2356\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.08987\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e5145\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e0.35315\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e0.32713\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e0.60231\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e0.29154\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e22.04933\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e0.26391\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e16.15072\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.04559\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n","\u003c/div\u003e\u003c/div\u003e\n","Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u003cbr/\u003eSynced \u003cstrong style=\"color:#cdcd00\"\u003eexp015_fold_3\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/3m68g9n4\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/3m68g9n4\u003c/a\u003e\u003cbr/\u003e\n","Find logs at: \u003ccode\u003e./wandb/run-20211108_190014-3m68g9n4/logs\u003c/code\u003e\u003cbr/\u003e\n"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","Some weights of the model checkpoint at deepset/xlm-roberta-large-squad2 were not used when initializing XLMRobertaModel: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ffad0e8928d4675b59d8a6a964b7d63","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/11074 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"315592a9e1d84fb1a02675838459446b","version_major":2,"version_minor":0},"text/plain":["[get features]:   0%|          | 0/222 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":["Number of train features: 23426, Number of valid features: 2662\n","Total Training Steps: 5206, Total Warmup Steps: 520\n"]},{"data":{"text/html":["\n","                    Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/2suivvvy\" target=\"_blank\"\u003eexp015_fold_4\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\"\u003edocs\u003c/a\u003e).\u003cbr/\u003e\n","\n","                "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","  | Name               | Type             | Params\n","--------------------------------------------------------\n","0 | model              | ChaiiModel       | 559 M \n","1 | start_criterion    | CrossEntropyLoss | 0     \n","2 | end_criterion      | CrossEntropyLoss | 0     \n","3 | train_mean_metric1 | MeanMetric       | 0     \n","4 | train_mean_metric2 | MeanMetric       | 0     \n","5 | valid_mean_metric1 | MeanMetric       | 0     \n","6 | valid_mean_metric2 | MeanMetric       | 0     \n","7 | lovasz_hinge       | LovaszHingeLoss  | 0     \n","--------------------------------------------------------\n","559 M     Trainable params\n","0         Non-trainable params\n","559 M     Total params\n","2,239.570 Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab/kaggle/chaii-hindi-and-tamil-question-answering/checkpoint/exp015 exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6865f6ebcdce4fafb13a8468f1aee225","version_major":2,"version_minor":0},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Global seed set to 1234\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e462beb071446219e1003c270fe60a9","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f74438f3ae0a469aad7c833a6f96b82f","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0345bbbb6a1f4966b47cd2d1fc461070","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cbr/\u003eWaiting for W\u0026B process to finish, PID 3220... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa9ec0f89e04419297ab39e71ae0db2b","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\u003cstyle\u003e\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    \u003c/style\u003e\n","\u003cdiv class=\"wandb-row\"\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun history:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e████▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▁▁▁▁▁▅▅▅▅▆▆▆▆▇▇▇▇▇██▁▂▂▂▂\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e▁█\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e█▁\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\u003cbr/\u003e\u003c/div\u003e\u003cdiv class=\"wandb-col\"\u003e\n","\u003ch3\u003eRun summary:\u003c/h3\u003e\u003cbr/\u003e\u003ctable class=\"wandb\"\u003e\u003ctr\u003e\u003ctd\u003eepoch\u003c/td\u003e\u003ctd\u003e1\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg1\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003elr-AdamW/pg2\u003c/td\u003e\u003ctd\u003e0.0\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e2.28216\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/bce_loss_step\u003c/td\u003e\u003ctd\u003e1.95059\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_end_epoch\u003c/td\u003e\u003ctd\u003e1.22092\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_epoch\u003c/td\u003e\u003ctd\u003e52.86434\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_start_epoch\u003c/td\u003e\u003ctd\u003e1.03308\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/loss_step\u003c/td\u003e\u003ctd\u003e38.63187\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrain/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.08877\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003etrainer/global_step\u003c/td\u003e\u003ctd\u003e5205\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_epoch\u003c/td\u003e\u003ctd\u003e0.37955\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/bce_loss_step\u003c/td\u003e\u003ctd\u003e0.35482\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/jaccard_epoch\u003c/td\u003e\u003ctd\u003e0.63524\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_end_epoch\u003c/td\u003e\u003ctd\u003e0.31623\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_epoch\u003c/td\u003e\u003ctd\u003e25.43691\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_start_epoch\u003c/td\u003e\u003ctd\u003e0.29205\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/loss_step\u003c/td\u003e\u003ctd\u003e18.61427\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd\u003eval/lovasz_hinge_epoch\u003c/td\u003e\u003ctd\u003e1.04226\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n","\u003c/div\u003e\u003c/div\u003e\n","Synced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u003cbr/\u003eSynced \u003cstrong style=\"color:#cdcd00\"\u003eexp015_fold_4\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/2suivvvy\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/kaggle-chaii-hindi-and-tamil-question-answering/runs/2suivvvy\u003c/a\u003e\u003cbr/\u003e\n","Find logs at: \u003ccode\u003e./wandb/run-20211108_201253-2suivvvy/logs\u003c/code\u003e\u003cbr/\u003e\n"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CV jaccard score : 23.58043899536133\n"]}],"source":["# tokenizer = AutoTokenizer.from_pretrained(cfg.tokenizer_name)\n","# _, _ = run_fold(cfg, train_df, 0, tokenizer)\n","run_training(cfg, train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MbsE5t9-jbB3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting upload for file exp015-fold-0-epoch=1.ckpt\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2.09G/2.09G [01:15\u003c00:00, 29.6MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp015-fold-0-epoch=1.ckpt (2GB)\n","Starting upload for file exp015-fold-1-epoch=1.ckpt\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2.09G/2.09G [01:16\u003c00:00, 29.2MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp015-fold-1-epoch=1.ckpt (2GB)\n","Starting upload for file exp015-fold-2-epoch=1.ckpt\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2.09G/2.09G [00:57\u003c00:00, 38.9MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp015-fold-2-epoch=1.ckpt (2GB)\n","Starting upload for file exp015-fold-3-epoch=1.ckpt\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2.09G/2.09G [01:16\u003c00:00, 29.3MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp015-fold-3-epoch=1.ckpt (2GB)\n","Starting upload for file exp015-fold-4-epoch=1.ckpt\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2.09G/2.09G [01:15\u003c00:00, 29.5MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp015-fold-4-epoch=1.ckpt (2GB)\n"]}],"source":["ID = \"azupero\"\n","DATASET_ID = f\"chaii-qa-checkpoint-{cfg.exp_name}\"\n","checkpoint_path = cfg.checkpoint_dir + \"/\" + cfg.exp_name\n","UPLOAD_DIR = Path(checkpoint_path)\n","\n","def dataset_create_new():\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = f'{ID}/{DATASET_ID}'\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = DATASET_ID\n","    with open(UPLOAD_DIR / 'dataset-metadata.json', 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","    api = KaggleApi()\n","    api.authenticate()\n","    api.dataset_create_new(folder=UPLOAD_DIR, convert_to_csv=False, dir_mode='tar')\n","\n","dataset_create_new()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPzqSOxpqgY6rOrDR4rc+QH","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1-pi6peACg5_Ax8JsQpU0Y1KSIyj5MSF0","name":"exp015_xlmroberta.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0072dd6ab5884342a63898e51411a158":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"105c54bdd85a4d48b534222d1e1f6299":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_965eaae559d847d3a73f71ef2b6cd888","placeholder":"​","style":"IPY_MODEL_6e8de21faa3b46d7b32a59c98d2fad64","value":" 20/8050 [00:05\u0026lt;38:43,  3.46it/s, loss=5.98e+03, v_num=ibxz, train/loss_step=5.98e+3, train/bce_loss_step=6.190]"}},"125fbc51d1a24ab5b95798fbde82be2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c332b38c0d2f450793d113d48e53ab6a","placeholder":"​","style":"IPY_MODEL_12662de321784faa874976f544b24d67","value":" 223/223 [00:14\u0026lt;00:00, 18.23it/s]"}},"12662de321784faa874976f544b24d67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"170390c71a3f434783130673917227fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b454cca69f94ac3abc84bd5e06d94fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e5304b9f37e427e8654475018d56c67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eeef3c69f2f4de9965a1466e6eb8b8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20a1282f581a457c9efccaa7d6e73b4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b0edad0e4fe408089405a584d749372","placeholder":"​","style":"IPY_MODEL_589c9bb7d6394a47a619329f02abf8f6","value":"Epoch 0:   0%"}},"2ca67d911f6c407d84e06a9610f92a08":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"2ea9d6291c1845cd9ba49fe9900fad06":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"300182cf439e4183bf43db1f465d947d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"339c93391f8c4fff835306b167967a66":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b454cca69f94ac3abc84bd5e06d94fc","placeholder":"​","style":"IPY_MODEL_2ea9d6291c1845cd9ba49fe9900fad06","value":" 11073/11073 [02:54\u0026lt;00:00, 79.22it/s]"}},"393231648825485f9aae19c905f1c46b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c50d6eb79c27493aa8dcf7e32f075331","IPY_MODEL_6665c203f2c34ce2b85a0be119d90730","IPY_MODEL_339c93391f8c4fff835306b167967a66"],"layout":"IPY_MODEL_0072dd6ab5884342a63898e51411a158"}},"3a5c14af141143459df4e6d78001e9e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ee675a4a154498c8d1407566465e716":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58821beec2054f1a92472551318d473d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"589c9bb7d6394a47a619329f02abf8f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e2b7f5593844b149fe788ac235f8199":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e987ed7cedc4020856c93342562e388":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6665c203f2c34ce2b85a0be119d90730":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8604d14e3d5545a4b70ffe4af4ef1a6f","max":11073,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cbc60e87aa9045039a5c09662f8a73ce","value":11073}},"6b0edad0e4fe408089405a584d749372":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c43b2a51c7c4d758ab52216da7523dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e8de21faa3b46d7b32a59c98d2fad64":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8604d14e3d5545a4b70ffe4af4ef1a6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88d52539d6314f4e9f8acc7b4b627b82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af5f1c36c236479ba54da9686913938a","placeholder":"​","style":"IPY_MODEL_5e987ed7cedc4020856c93342562e388","value":"[get features]: 100%"}},"9000bbfa49ea4d56a54292fc44025d50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91aa3956ebaf4804b3e9bae36a2fc477","placeholder":"​","style":"IPY_MODEL_c9b2c7fff5e84cf7b35337803345cf55","value":" 0/2 [00:01\u0026lt;?, ?it/s]"}},"91aa3956ebaf4804b3e9bae36a2fc477":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"965eaae559d847d3a73f71ef2b6cd888":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97253b834bea4ba79a352497c2341340":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"a4d53a69fbc74cef8d56d874ad58cf5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e5304b9f37e427e8654475018d56c67","max":223,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1eeef3c69f2f4de9965a1466e6eb8b8a","value":223}},"af5f1c36c236479ba54da9686913938a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2427c9da54248a9a42406999f65af9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c43b2a51c7c4d758ab52216da7523dd","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58821beec2054f1a92472551318d473d","value":0}},"c0dec382071f48a997a7d6cb8e1c5800":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ee675a4a154498c8d1407566465e716","placeholder":"​","style":"IPY_MODEL_eeb723c2e04648ea9585efd8243d0982","value":"Validation sanity check:   0%"}},"c332b38c0d2f450793d113d48e53ab6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c50d6eb79c27493aa8dcf7e32f075331":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_170390c71a3f434783130673917227fc","placeholder":"​","style":"IPY_MODEL_ee4fff933a8840fba7d63d6a67103e4a","value":"[get features]: 100%"}},"c9b2c7fff5e84cf7b35337803345cf55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbc60e87aa9045039a5c09662f8a73ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfb556ab3dcd4a90a646f8569f29626c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88d52539d6314f4e9f8acc7b4b627b82","IPY_MODEL_a4d53a69fbc74cef8d56d874ad58cf5b","IPY_MODEL_125fbc51d1a24ab5b95798fbde82be2f"],"layout":"IPY_MODEL_3a5c14af141143459df4e6d78001e9e8"}},"dd1d77016c644afc990dcc69e8f4f8f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20a1282f581a457c9efccaa7d6e73b4d","IPY_MODEL_fdcd52456b6c4929ba64152dc3e785f2","IPY_MODEL_105c54bdd85a4d48b534222d1e1f6299"],"layout":"IPY_MODEL_97253b834bea4ba79a352497c2341340"}},"ee4fff933a8840fba7d63d6a67103e4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee5352ce6f294519896e841b0d4d1d5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c0dec382071f48a997a7d6cb8e1c5800","IPY_MODEL_b2427c9da54248a9a42406999f65af9f","IPY_MODEL_9000bbfa49ea4d56a54292fc44025d50"],"layout":"IPY_MODEL_2ca67d911f6c407d84e06a9610f92a08"}},"eeb723c2e04648ea9585efd8243d0982":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdcd52456b6c4929ba64152dc3e785f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e2b7f5593844b149fe788ac235f8199","max":8050,"min":0,"orientation":"horizontal","style":"IPY_MODEL_300182cf439e4183bf43db1f465d947d","value":20}}}}},"nbformat":4,"nbformat_minor":0}